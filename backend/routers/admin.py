"""
ç®¡ç†å‘˜å†…å®¹ç®¡ç†è·¯ç”±ï¼ˆç®€åŒ–ç‰ˆ - ä»¥è‰ç¨¿ä¸ºä¸»ï¼‰
"""
from fastapi import APIRouter, HTTPException, Depends
from typing import List
from pathlib import Path
import json
from datetime import datetime
from backend.routers.auth import get_current_admin

router = APIRouter()

# æ•°æ®ç›®å½•
ADMIN_DATA_DIR = Path(__file__).parent.parent.parent / "admin_data"
DRAFTS_DIR = ADMIN_DATA_DIR / "drafts"
PUBLISHED_DIR = ADMIN_DATA_DIR / "published"
IMAGES_DIR = ADMIN_DATA_DIR / "images"

def ensure_dirs():
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    ADMIN_DATA_DIR.mkdir(parents=True, exist_ok=True)
    DRAFTS_DIR.mkdir(parents=True, exist_ok=True)
    PUBLISHED_DIR.mkdir(parents=True, exist_ok=True)

def get_draft_path(content_type: str) -> Path:
    """è·å–è‰ç¨¿æ–‡ä»¶è·¯å¾„"""
    if content_type not in ['research', 'media', 'activity', 'shop', 'announcement']:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„å†…å®¹ç±»å‹")
    return DRAFTS_DIR / f"{content_type}.json"

def get_content_path(content_type: str) -> Path:
    """è·å–æ­£æ–‡å‘å¸ƒæ–‡ä»¶è·¯å¾„"""
    if content_type not in ['research', 'media', 'activity', 'shop', 'announcement']:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„å†…å®¹ç±»å‹")
    return PUBLISHED_DIR / f"{content_type}.json"

def read_json(file_path: Path) -> dict:
    """è¯»å–JSONæ–‡ä»¶"""
    if not file_path.exists():
        return {"posts": []}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return {"posts": []}

def write_json(file_path: Path, data: dict):
    """å†™å…¥JSONæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

@router.get("/{content_type}")
async def get_all_drafts(
    content_type: str,
    admin: str = Depends(get_current_admin)
):
    """
    è·å–æ‰€æœ‰è‰ç¨¿ï¼ˆåŒ…æ‹¬ draft å’Œ published çŠ¶æ€ï¼‰
    ç®¡ç†å‘˜çœ‹åˆ°æ‰€æœ‰å†…å®¹
    """
    draft_path = get_draft_path(content_type)
    data = read_json(draft_path)
    return data.get("posts", [])

@router.post("/{content_type}")
async def save_draft(
    content_type: str,
    post_data: dict,
    admin: str = Depends(get_current_admin)
):
    """
    ä¿å­˜è‰ç¨¿ï¼ˆæ–°å»ºæˆ–æ›´æ–°ï¼‰
    å¦‚æœçŠ¶æ€æ˜¯ draftï¼ŒåŒæ—¶ä»æ­£æ–‡ä¸­åˆ é™¤ï¼ˆæ’¤é”€å‘å¸ƒï¼‰
    """
    ensure_dirs()
    draft_path = get_draft_path(content_type)
    content_path = get_content_path(content_type)
    
    data = read_json(draft_path)
    posts = data.get("posts", [])
    
    # æ·»åŠ æ—¶é—´æˆ³
    post_data['updated_at'] = datetime.now().isoformat()
    if 'created_at' not in post_data:
        post_data['created_at'] = datetime.now().isoformat()
    if 'id' not in post_data:
        post_data['id'] = datetime.now().strftime('%Y%m%d%H%M%S%f')
    
    # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨
    existing_index = None
    for i, post in enumerate(posts):
        if post.get('id') == post_data['id']:
            existing_index = i
            break
    
    # ğŸ”¥ å¦‚æœæ˜¯æ›´æ–°æ“ä½œï¼Œåˆ é™¤è¢«ç§»é™¤çš„å›¾ç‰‡
    if existing_index is not None:
        old_post = posts[existing_index]
        old_images = set(old_post.get('images', []))
        new_images = set(post_data.get('images', []))
        removed_images = old_images - new_images
        
        for img_url in removed_images:
            filename = img_url.split('/')[-1]
            (IMAGES_DIR / filename).unlink(missing_ok=True)
    
    # æ›´æ–°æˆ–æ·»åŠ 
    if existing_index is not None:
        posts[existing_index] = post_data
    else:
        posts.insert(0, post_data)
    
    data['posts'] = posts
    write_json(draft_path, data)
    
    # å¦‚æœä¿å­˜ä¸ºè‰ç¨¿çŠ¶æ€ï¼Œä»æ­£æ–‡ä¸­åˆ é™¤ï¼ˆæ’¤é”€å‘å¸ƒï¼‰
    if post_data.get('status') == 'draft':
        content_data = read_json(content_path)
        content_posts = content_data.get("posts", [])
        # åˆ é™¤æ­£æ–‡ä¸­å¯¹åº”çš„æ–‡ç« 
        content_posts = [p for p in content_posts if p.get('id') != post_data['id']]
        content_data['posts'] = content_posts
        write_json(content_path, content_data)
    
    return post_data

@router.post("/{content_type}/{post_id}/publish")
async def publish_draft(
    content_type: str,
    post_id: str,
    admin: str = Depends(get_current_admin)
):
    """
    å‘å¸ƒè‰ç¨¿ï¼š
    1. æ›´æ–°è‰ç¨¿çŠ¶æ€ä¸º published
    2. å¤åˆ¶åˆ°æ­£æ–‡åŒº
    """
    ensure_dirs()
    draft_path = get_draft_path(content_type)
    content_path = get_content_path(content_type)
    
    # è¯»å–è‰ç¨¿
    draft_data = read_json(draft_path)
    posts = draft_data.get("posts", [])
    
    # æ‰¾åˆ°è¦å‘å¸ƒçš„æ–‡ç« 
    post_to_publish = None
    for post in posts:
        if post.get('id') == post_id:
            post['status'] = 'published'
            post['published_at'] = datetime.now().isoformat()
            post_to_publish = post
            break
    
    if not post_to_publish:
        raise HTTPException(status_code=404, detail="è‰ç¨¿ä¸å­˜åœ¨")
    
    # ä¿å­˜æ›´æ–°åçš„è‰ç¨¿
    write_json(draft_path, draft_data)
    
    # å¤åˆ¶åˆ°æ­£æ–‡
    content_data = read_json(content_path)
    content_posts = content_data.get("posts", [])
    
    # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥IDçš„æ­£æ–‡
    existing_index = None
    for i, post in enumerate(content_posts):
        if post.get('id') == post_id:
            existing_index = i
            break
    
    # æ›´æ–°æˆ–æ·»åŠ åˆ°æ­£æ–‡
    if existing_index is not None:
        content_posts[existing_index] = post_to_publish
    else:
        content_posts.insert(0, post_to_publish)
    
    content_data['posts'] = content_posts
    write_json(content_path, content_data)
    
    return {"success": True, "message": "å‘å¸ƒæˆåŠŸ"}

@router.post("/{content_type}/{post_id}/edit")
async def edit_post(
    content_type: str,
    post_id: str,
    admin: str = Depends(get_current_admin)
):
    """
    ç¼–è¾‘æ–‡ç« ï¼ˆä»¥è‰ç¨¿ä¸ºä¸»ï¼‰ï¼š
    1. ä»è‰ç¨¿ä¸­æŸ¥æ‰¾è¦ç¼–è¾‘çš„æ–‡ç« 
    2. ç«‹å³ä»æ­£æ–‡ä¸­åˆ é™¤è¯¥æ–‡ç« ï¼ˆé¿å…é‡å¤æ˜¾ç¤ºï¼‰
    3. ä¿æŒè‰ç¨¿ä¸å˜ï¼Œè®©ç”¨æˆ·ç¼–è¾‘
    """
    ensure_dirs()
    draft_path = get_draft_path(content_type)
    content_path = get_content_path(content_type)
    
    # è¯»å–è‰ç¨¿å’Œæ­£æ–‡
    draft_data = read_json(draft_path)
    content_data = read_json(content_path)
    
    # ä»è‰ç¨¿ä¸­æŸ¥æ‰¾è¦ç¼–è¾‘çš„æ–‡ç« ï¼ˆè‰ç¨¿æ˜¯ä¸»æ•°æ®æºï¼‰
    post_to_edit = None
    for post in draft_data.get("posts", []):
        if post.get('id') == post_id:
            post_to_edit = post.copy()
            break
    
    if not post_to_edit:
        raise HTTPException(status_code=404, detail="è‰ç¨¿ä¸å­˜åœ¨")
    
    # ç«‹å³ä»æ­£æ–‡ä¸­åˆ é™¤è¯¥æ–‡ç« ï¼ˆé¿å…é‡å¤æ˜¾ç¤ºï¼‰
    content_posts = content_data.get("posts", [])
    content_posts = [p for p in content_posts if p.get('id') != post_id]
    content_data['posts'] = content_posts
    write_json(content_path, content_data)
    
    # æ›´æ–°è‰ç¨¿ä¸­çš„æ–‡ç« çŠ¶æ€ä¸º draftï¼ˆç¡®ä¿çŠ¶æ€æ­£ç¡®ï¼‰
    post_to_edit['status'] = 'draft'
    post_to_edit['updated_at'] = datetime.now().isoformat()
    
    # æ›´æ–°è‰ç¨¿ä¸­çš„æ–‡ç« 
    draft_posts = draft_data.get("posts", [])
    for i, post in enumerate(draft_posts):
        if post.get('id') == post_id:
            draft_posts[i] = post_to_edit
            break
    
    draft_data['posts'] = draft_posts
    write_json(draft_path, draft_data)
    
    return {"success": True, "message": "å·²è¿›å…¥ç¼–è¾‘æ¨¡å¼ï¼Œæ–‡ç« å·²ä»æ­£æ–‡ä¸­ç§»é™¤"}

@router.delete("/{content_type}/{post_id}")
async def delete_draft(
    content_type: str,
    post_id: str,
    admin: str = Depends(get_current_admin)
):
    """
    åˆ é™¤è‰ç¨¿ï¼š
    1. åˆ é™¤è‰ç¨¿
    2. åŒæ—¶åˆ é™¤å¯¹åº”çš„æ­£æ–‡
    3. åŒæ­¥åˆ é™¤å…³è”çš„å›¾ç‰‡æ–‡ä»¶
    """
    ensure_dirs()
    draft_path = get_draft_path(content_type)
    content_path = get_content_path(content_type)
    
    # ä»è‰ç¨¿ä¸­åˆ é™¤ï¼ˆå…ˆæ‰¾åˆ°æ–‡ç« å¹¶åˆ é™¤å›¾ç‰‡ï¼‰
    draft_data = read_json(draft_path)
    posts = draft_data.get("posts", [])
    
    # ğŸ”¥ åˆ é™¤å›¾ç‰‡æ–‡ä»¶
    post_to_delete = next((p for p in posts if p.get('id') == post_id), None)
    if post_to_delete and post_to_delete.get('images'):
        for img_url in post_to_delete['images']:
            filename = img_url.split('/')[-1]
            (IMAGES_DIR / filename).unlink(missing_ok=True)
    
    posts = [p for p in posts if p.get('id') != post_id]
    draft_data['posts'] = posts
    write_json(draft_path, draft_data)
    
    # ä»æ­£æ–‡ä¸­åˆ é™¤
    content_data = read_json(content_path)
    content_posts = content_data.get("posts", [])
    content_posts = [p for p in content_posts if p.get('id') != post_id]
    content_data['posts'] = content_posts
    write_json(content_path, content_data)
    
    return {"success": True, "message": "åˆ é™¤æˆåŠŸ"}

@router.get("/cleanup/scan")
async def scan_unreferenced_images(admin: str = Depends(get_current_admin)):
    """
    æ‰«ææœªå¼•ç”¨å›¾ç‰‡ï¼ˆä¸åˆ é™¤ï¼‰
    """
    ensure_dirs()
    
    # æ‰«æç£ç›˜æ‰€æœ‰å›¾ç‰‡
    all_images = set()
    if IMAGES_DIR.exists():
        for ext in ['*.webp', '*.gif']:
            for img_file in IMAGES_DIR.glob(ext):
                all_images.add(img_file.name)
    
    # æ‰«ææ‰€æœ‰JSONä¸­å¼•ç”¨çš„å›¾ç‰‡
    referenced_images = set()
    
    # æ‰«æè‰ç¨¿
    for content_type in ['research', 'media', 'activity', 'shop', 'announcement']:
        draft_path = get_draft_path(content_type)
        draft_data = read_json(draft_path)
        for post in draft_data.get("posts", []):
            for img_url in post.get("images", []):
                filename = img_url.split('/')[-1]
                referenced_images.add(filename)
    
    # æ‰«ææ­£æ–‡
    for content_type in ['research', 'media', 'activity', 'shop', 'announcement']:
        content_path = get_content_path(content_type)
        content_data = read_json(content_path)
        for post in content_data.get("posts", []):
            for img_url in post.get("images", []):
                filename = img_url.split('/')[-1]
                referenced_images.add(filename)
    
    # è®¡ç®—æœªå¼•ç”¨å›¾ç‰‡
    unreferenced = all_images - referenced_images
    
    # è·å–è¯¦ç»†ä¿¡æ¯
    unreferenced_details = []
    total_size = 0
    for filename in unreferenced:
        file_path = IMAGES_DIR / filename
        if file_path.exists():
            size = file_path.stat().st_size
            total_size += size
            unreferenced_details.append({
                "filename": filename,
                "size": size,
                "size_mb": round(size / (1024 * 1024), 2)
            })
    
    return {
        "total_images": len(all_images),
        "referenced_images": len(referenced_images),
        "unreferenced_count": len(unreferenced),
        "unreferenced_details": unreferenced_details,
        "total_size": total_size,
        "total_size_mb": round(total_size / (1024 * 1024), 2)
    }

@router.post("/cleanup/execute")
async def cleanup_unreferenced_images(admin: str = Depends(get_current_admin)):
    """
    æ‰§è¡Œæ¸…ç†æœªå¼•ç”¨å›¾ç‰‡
    """
    ensure_dirs()
    
    # æ‰«æç£ç›˜æ‰€æœ‰å›¾ç‰‡
    all_images = set()
    if IMAGES_DIR.exists():
        for ext in ['*.webp', '*.gif']:
            for img_file in IMAGES_DIR.glob(ext):
                all_images.add(img_file.name)
    
    # æ‰«ææ‰€æœ‰JSONä¸­å¼•ç”¨çš„å›¾ç‰‡
    referenced_images = set()
    
    # æ‰«æè‰ç¨¿
    for content_type in ['research', 'media', 'activity', 'shop', 'announcement']:
        draft_path = get_draft_path(content_type)
        draft_data = read_json(draft_path)
        for post in draft_data.get("posts", []):
            for img_url in post.get("images", []):
                filename = img_url.split('/')[-1]
                referenced_images.add(filename)
    
    # æ‰«ææ­£æ–‡
    for content_type in ['research', 'media', 'activity', 'shop', 'announcement']:
        content_path = get_content_path(content_type)
        content_data = read_json(content_path)
        for post in content_data.get("posts", []):
            for img_url in post.get("images", []):
                filename = img_url.split('/')[-1]
                referenced_images.add(filename)
    
    # è®¡ç®—æœªå¼•ç”¨å›¾ç‰‡
    unreferenced = all_images - referenced_images
    
    # åˆ é™¤æœªå¼•ç”¨å›¾ç‰‡
    deleted_count = 0
    freed_space = 0
    
    for filename in unreferenced:
        file_path = IMAGES_DIR / filename
        if file_path.exists():
            size = file_path.stat().st_size
            file_path.unlink()
            deleted_count += 1
            freed_space += size
    
    return {
        "success": True,
        "deleted_count": deleted_count,
        "freed_space": freed_space,
        "freed_space_mb": round(freed_space / (1024 * 1024), 2)
    }
